{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samuelwang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samuelwang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import ssl\n",
    "import heapq\n",
    "import re\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_json('Grocery_and_Gourmet_Food_Reviews_training.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textOnly = training[['overall', 'reviewText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "textOnly.dropna();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "textOnlyRatings = textOnly['overall']\n",
    "del textOnly['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textOnly is a dataframe with only text of review\n",
    "# textOnlyRatings is a dataframe with only rating of review.\n",
    "# Each index in textOnly corresponds to the same index in textOnlyRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Can't wait to get cooking with these. I have heard about capers and had never tasted them. However aster tasting them I can't wait to get experimenting with some different recipes.\",\n",
       "       'Large and good tasting', 'wonderful', ...,\n",
       "       \"Quinine bitterness not the least bit detectable. In fact not bitter even when drunk straight up.. Overwhelming sourness from all the citric acid. May make a decent albeit pricey whiskey sour. Will neither buy again nor recommend it to anyone. Grocery store tonics are far far better at a small fraction of the price. C'est la vie!\",\n",
       "       \"i guess i'm so used to regular tonic, this did not impress me that much, even though it gets great reviews. thought i'd try it at least once.\",\n",
       "       \"Maybe this works when added to a drink, but I wanted to use it to make tonic water and the result tasted so bad when I added it to some soda water I made on my Sodastream that I couldn't drink it!  The remainder of the bottle of concentrate went down the drain.  Now I'm back to buying high-fructose-sweetened Schweppes and mixing it 1:1 with soda water to get something with reasonable flavor and not too sweet.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textOnly.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = textOnly.values.flatten()[:100000] \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = str(dataset[i]).lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can t wait to get cooking with these i have heard about capers and had never tasted them however aster tasting them i can t wait to get experimenting with some different recipes '\n",
      " 'large and good tasting' 'wonderful' ...\n",
      " 'great never had these before now i eat them when i need a chocolate fix as they are big oh yeah and for baking too he he great nestle flavor yummy'\n",
      " 'love dark chocolate morsels right out of the package in my cookies cakes melted for decorating '\n",
      " 'absolutely delicious makes the best chocolate chip cookies my order came in perfect shape no issues some commenters said their order arrived melted mine did not it s worth noting these chips are a bit larger than the regular semi sweet chips which makes them a bit better if you ask me ']\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {} \n",
    "stop = stopwords.words('english')\n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in stop:\n",
    "            if word not in word2count.keys(): \n",
    "                word2count[word] = 1\n",
    "            else: \n",
    "                word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
    "# 100 says that we should only pay attention to the top 100 most frequent words. Change as youd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create word count vector for each review\n",
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000 Number of Features: 100\n"
     ]
    }
   ],
   "source": [
    "# store awesome or not for each review within Y\n",
    "Y = []\n",
    "scores = textOnlyRatings.values.flatten()[:100000] \n",
    "for score in scores:\n",
    "    Y.append(score > 4.4) # awesome or not is determined by whether score is greater than 4.4\n",
    "Y = np.asarray(Y)\n",
    "df = pd.DataFrame(X) \n",
    "print(\"Number of samples: {} Number of Features: {}\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6904242424242424"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model, fit, and score\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, Y_train)\n",
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
